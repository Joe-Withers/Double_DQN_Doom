from random import choice
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import random
from replay_buffer import Replay_Buffer
import os

class DQN_Agent():
    def __init__(self, state_size, action_size, batch_size=6, learning_rate=0.001, replay_buffer_size=10000, checkpoint_file=None):
        self.action_size = action_size
        print('self.action_size',self.action_size)
        self.q_network = self.create_q_network(learning_rate)
        if checkpoint_file is not None:
            checkpoint_folder, _ = os.path.split(checkpoint_file)
            checkpoint_path = checkpoint_folder+"/cp-{epoch:04d}.ckpt"
            try:
                checkpnt = tf.train.latest_checkpoint(checkpoint_folder)
                self.q_network.load_weights(checkpnt)
            except:
                print('****************no checkpoint file to load**********************')
        else:
            checkpoint_path = "autogenerated"+str(random.randint(1,1000))+"/cp-{epoch:04d}.ckpt"
        self.cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, verbose=1, save_weights_only=True)
        self.q_target_network = self.create_q_network(learning_rate)
        self.soft_target_weight_update(1)
        #parameters
        self.df = 0.99 # discount factor
        self.tau = 0.95
        self.batch_size = batch_size
        #replay buffer
        self.replay_buffer = Replay_Buffer(replay_buffer_size)

    def create_q_network(self, learning_rate):
        q_network = Sequential()
        q_network.add(Conv2D(32, kernel_size=8, activation='relu'))
        q_network.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
        q_network.add(Conv2D(64, kernel_size=4, activation='relu'))
        q_network.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
        q_network.add(Conv2D(128, kernel_size=4, activation='relu'))
        q_network.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
        q_network.add(Flatten())
        q_network.add(Dense(128, activation='relu'))
        q_network.add(Dense(self.action_size, activation='softmax'))
        q_network.compile(loss='mse', optimizer=Adam(lr=learning_rate))
        return q_network


    def select_epsilon_greedy_action(self,state,epsilon):
        if np.random.uniform(0, 1) < epsilon:
            return choice(np.arange(0,self.action_size))  # random action
        else:
            q_values = self.q_network.predict(state)
            return np.argmax(q_values[0])  # greedy action

    def soft_target_weight_update(self,tau):
        weights = np.asarray(self.q_network.get_weights())
        target_weights = np.asarray(self.q_target_network.get_weights())
        self.q_target_network.set_weights(weights * tau + target_weights * (1 - tau))

    def learn_from_transition(self,action,state,next_state,reward,done_bool):
        q_values = self.q_network.predict(state)
        target_q_values = q_values
        target_network_q_next_state = self.q_target_network.predict(next_state)
        network_q_next_state = self.q_network.predict(next_state)
        target_q_values[0][action] = reward + done_bool * self.df * target_network_q_next_state[0][np.argmax(network_q_next_state)]
        return target_q_values[0]

    def learn_from_transitions(self,actions,states,next_states,rewards,done_bools):
        target_q_values = np.array([self.learn_from_transition(action,state,next_state,reward,done_bool) for action,state,next_state,reward,done_bool in zip(actions,states,next_states,rewards,done_bools)])
        states = np.array(states)
        states = np.squeeze(states, axis=1)
        self.q_network.fit(states, target_q_values, epochs=1, batch_size=self.batch_size, verbose=0, callbacks=[self.cp_callback])

    def learn_from_m_random_transitions_in_replay_buffer(self,m):
        action,state,next_state,reward,done_bool = self.replay_buffer.get_sample(m)
        self.learn_from_transitions(action, state, next_state, reward, done_bool)
        self.soft_target_weight_update(0.5)

    def add_replay(self,replay):
        self.replay_buffer.add_replay(replay)
